
---
title: "Module 9: Probit Regression"
author: "Rebecca C. Steorts"
output: 
     beamer_presentation:
      includes: 
          in_header: custom2.tex
font-size: 8px
---


Agenda
===
- Ordinal, numeric, and continous variables
- Probit versus linear regression
- Full conditionals
- An application to the 1994 General Social Survey


Generalized Linear Regression
===

- Many datasets include variables whose distributions cannot be represented by the normal, binomial or Poisson distributions we have studied so far.
- Distributions of common survey variables such as age, education level and income generally cannot be accurately described the above sampling models. 
- In this module, we will use the probit regression model to handle such cases. 

Terminology
===

- We use the term ordinal to refer to any variable for which there is a logical ordering of the sample space. 

- We use the term numeric to refer to variables that have meaningful numerical scales. 

- We use the term continuous if a variable can have a value that is (roughly) any real number in an interval.

Data
===

- The 1994 General Social Survey provides data on many variables in households (United States).

- There are variables such as the following:
1. DEG (highest degree obtained by individual)
2. CHILD (the number of children in a household)
3. PDEG (binary indicator of whether or not either parent obtained a college degree)

- Using these data, we might be tempted to investigate the relationship between the variables with a linear regression model.

- This is problematic due to the ordinal nature of the data. 

Data
===
\footnotesize
```{r}
dat<-read.table("data/social-survey.dat",
                header=TRUE)
head(dat)
```

Two ordinal variables having non-normal distributions
===
```{r, echo=FALSE}
yincc<-match(dat$INC,sort(unique(dat$INC)))
ydegr<-dat$DEGREE+1
yage<-dat$AGE
ychild<-dat$CHILD
ypdeg<-1*(dat$PDEG>2)
tmp<-lm(ydegr~ychild+ypdeg+ychild:ypdeg)
```

```{r, echo=FALSE}
par(mar=c(3,3,1,1),mgp=c(1.25,.5,0))
par(mfrow=c(1,2))
plot(table(dat$DEG+1)/sum(table(dat$DEG+1)),
  lwd=2,type="h",xlab="DEG",ylab="probability")
plot(table(dat$CHILD)/sum(table(dat$CHILD)),lwd=2,type="h",xlab="CHILD",ylab="probability" )
```

Probit regression
===

- Linear or generalized linear regression models, which assume a numeric scale to the data, may be appropriate for variables like height or weight, but are not appropriate for non-numeric ordinal variables like DEG or CHILD.

- This idea motivates a modeling technique known as ordered probit regression.

- We relate the response $Y$ to a vector of predictors $x$ via a regression model using 
a latent variable $Z$.

Probit regression model
===
The model can be written as

\begin{align}
Y_i &= g(Z_i)\\
Z_i &= \beta^T x_i + \epsilon_i\\
\epsilon_i &\stackrel{iid}{\sim} Normal (0,1)\\
\beta &\sim MVN(0, n(X^TX)^{-1}),
\end{align}
where 
$g$ is any non-decreasing function.

Notation 
===

- $X_{n\times p}$: regression features or covariates (design matrix)
- $Z_{n\times 1}:$ latent variable 
- $\by_{n\times 1}$: response variable (vector)
- $\bbeta_{p \times 1}$: vector of regression coefficients

The role of g
===
\begin{align}
Y_{n \times 1} &= g(Z)\\
Z_{n \times 1} &= X_{n \times p} \beta_{p \times 1} + \epsilon_{n \times 1}\\
\epsilon_{n \times 1} &\stackrel{iid}{\sim} Normal (0,I)\\
\beta_{p \times 1} &\sim MVN(0, n(X^TX)^{-1})
\end{align}

Suppose the sample space for $Y$ takes on $K$ values $\{1,2, \ldots,K\}$, then $g$ can be described with $K-1$ ordered parameters. 

You can think of the values of $g_1, \ldots g_{K-1}$ as thresholds so that moving past $z$ will move $y$ into the next (highest) category. 

Full conditional of $\beta$
===
\begin{align}
Y_{n \times 1} \mid Z &= g(Z)\\
Z_{n \times 1}\mid \bbeta &\sim MVN(X\bbeta, I)\\
\beta_{p \times 1} &\sim MVN(0, n(X^TX)^{-1})
\end{align}


$$p( \beta \mid y, z, g) \propto p(\beta) p(z \mid \beta)$$

Using the MVN conjugacy that we looked at before, $\beta \mid y, z, g$ will be MVN where

$$E[\beta \mid z]= \frac{n}{n+1} (X^TX)^{-1}X^T z$$
$$Var[\beta \mid z] = \frac{n}{n+1} (X^TX)^{-1}$$



Full conditional of $Z$
===

Under the sampling model, the conditional distribution of 
$$Z_i \mid \beta \sim \text{Normal}(\beta^Tx_i, 1)$$

Given g and observing $Y_i = y_i$, we know that $Z_i$ lies in the interval
$(g_{i-1}, g_i).$ 

Let $a=g_{i-1}, b=g_i.$ 

Then $$p(z_i \mid \beta, z, y, g) \propto \text{dnorm}(z_i, \beta^T x_i, 1) \times I_{a,b}(z_i)$$

This is simply a density of a constrained normal distribution. 
How to sample? Apply the inverse CDF trick that we have done previously! 

Full conditional of $g$
===
Suppose the prior distribution is $p(g)$. 

Given $Y=y$ and $Z=z$, then we know: 

- $g_k$ must be higher than all $z_i$'s for which $y_i = k$ and 
- $g_k$ must be lower than all $z_i$'s for which $y_i = k+1$

Let $a_k = \max\{z_i: y_i=k\}$ and $b_k = \min\{z_i: y_i = k+1\}.$ 

Then the full conditional distribution of $g$ is then proportional to $p(g)$ but constrained to the set $\{g: a_k < g_k < b_k\}.$


Application to the General Social Survey
===

Some researchers suggest that having children reduces opportunities for educational attainment (Moore and Waite, 1977). 

Here we examine this hypothesis in a sample of males in the labor force (meaning not retired, not in school, and not in an institution), obtained from the 1994 General Social Survey. 

For 959 of the 1,002 survey respondents we have complete data on the variables DEG, CHILD and PDEG described above. 

We have the following variables: 

- $Y_i = DEG_i$
- $x_i = (CHILD_i, PDEG_i, CHILD_i \times PDEG_i)$

Model
===
Our model specification is the following: 
\begin{align}
Y_{n \times 1} &= g(Z)\\
Z_{n \times 1} &= X_{n \times p} \beta_{p \times 1} + \epsilon\\
\epsilon_{n \times 1} &\stackrel{iid}{\sim} Normal (0,I)\\
\beta_{p \times 1} &\sim MVN(0, n(X^TX)^{-1})
\end{align}

$$p(g) \propto \prod_{k=1}^{K-1} (g_k,0,100)$$

Application to the General Social Survey
===
```{r}
X<-cbind(ychild,ypdeg,ychild*ypdeg)
head(X)
head(y<-ydegr)
```

Application to the General Social Survey
===
```{r}
# replacing missing values with the mean
keep<-(1:length(y))[ !is.na( apply( cbind(X,y),1,mean) ) ]
X<-X[keep,]
y<-y[keep]
```

Application to the General Social Survey
===
```{r}
## data without missing values
head(X)
## response without missing values
head(y)
```

Application to the General Social Survey
===
```{r}
## short calculations
n<-dim(X)[1] 
p<-dim(X)[2]
iXX<-solve(t(X)%*%X)
V<-iXX*(n/(n+1)) 
## review the cholesk decomposition
## if you have forgotten this from linear algebra
cholV <- chol(V)
```

Application to the General Social Survey
===
```{r}
# find the unique y
# then sort them
# then return the first occurence of 
# y in sort(unique(y))
ranks<-match(y,sort(unique(y)))
head(ranks)
# sort the ranks
uranks<-sort(unique(ranks))
head(uranks)
```

Application to the General Social Survey
===
```{r}
###starting values
set.seed(1)
(beta<-rep(0,p))
```

Initializing z
===

We know that $Z$ is Gaussian. We can rank the $y$'s (breaking ties at random). Then we can evaluation this with the \texttt{qnorm} function. 

```{r}
z<-qnorm(rank(y,ties.method="random")/(n+1))
head(z)
```

Other initializions
===
```{r}
(g<-rep(NA,length(uranks)-1))
K<-length(uranks)
BETA<-matrix(NA,1000,p)
Z<-matrix(NA,1000,n)
ac<-0
mu<-rep(0,K-1)
sigma<-rep(100,K-1)
S<-25000
```

Gibbs sampler
===
\tiny
```{r}
for(s in 1:S) {
  #update g 
  for(k in 1:(K-1)){
  a<-max(z[y==k])
  b<-min(z[y==k+1])
  u<-runif(1, pnorm( (a-mu[k])/sigma[k] ),
              pnorm( (b-mu[k])/sigma[k] ) )
  g[k]<- mu[k] + sigma[k]*qnorm(u)
  }

  #update beta
  E<- V%*%( t(X)%*%z )
  beta<- cholV%*%rnorm(p) + E

  #update z
  ez<-X%*%beta
  a<-c(-Inf,g)[ match( y-1, 0:K) ]
  b<-c(g,Inf)[y]  
  u<-runif(n, pnorm(a-ez),pnorm(b-ez) )
  z<- ez + qnorm(u)

  #help mixing
  c<-rnorm(1,0,n^(-1/3))  
  zp<-z+c ; gp<-g+c
  lhr<-  sum(dnorm(zp,ez,1,log=T) - dnorm(z,ez,1,log=T) ) + 
         sum(dnorm(gp,mu,sigma,log=T) - dnorm(g,mu,sigma,log=T) )
  if(log(runif(1))<lhr) { z<-zp ; g<-gp ; ac<-ac+1 }

  if(s%%(S/1000)==0){ 
    cat(s/S,ac/s,"\n")
    BETA[s/(S/1000),]<-  beta
    Z[s/(S/1000),]<- z
    }} 
```

Plot
===
```{r, echo=FALSE}
par(mar=c(3,3,1,1),mgp=c(1.75,.75,0))
par(mfrow=c(1,2))
plot(X[,1]+.25*(X[,2]),Z[1000,],
 pch=15+X[,2],col=c("gray","black")[X[,2]+1],
 xlab="number of children",ylab="z", ylim=range(c(-2.5,4,Z[1000,])),
    xlim=c(0,9))

beta.pm<-apply(BETA,2,mean)
ZPM<-apply(Z,2,mean)
abline(0,beta.pm[1],lwd=2 ,col="gray")
abline(beta.pm[2],beta.pm[1]+beta.pm[3],col="black",lwd=2 )
#legend(3.75,4.25,legend=c("parents without college","parents with college"),pch=c(15,16),col=c("gray","black"))
legend(5,4,legend=c("PDEG=0","PDEG=1"),pch=c(15,16),col=c("gray","black"))


plot(density(BETA[,3],adj=2),lwd=2,xlim=c(-.5,.5),main="",
    xlab=expression(beta[3]),ylab="density")
sd<-sqrt(  solve(t(X)%*%X/n)[3,3] )
x<-seq(-.7,.7,length=100)
lines(x,dnorm(x,0,sd),lwd=2,col="gray")
legend(-.5,6.5,legend=c("prior","posterior"),lwd=c(2,2),col=c("gray","black"),bty="n")
```

Commentary for Left Plot
===

The posterior mean regression line for people without a college-educated parent $(x_{i,2}=0)$ is 

$$E[Z \mid y, x_1, x_2=0] = -0.024 x_1.$$

The posterior mean regression line for people with a college-educated parent

$$E[Z \mid y, x_1, x_2=0] = 0.818 + 0.054 x_1.$$

In the above figure (left), we see that  for people whose parents did not go to college, the number of children they have is indeed weakly negatively associated with their educational outcome. (The opposite is true for people whose parents did go to college).

Commentary for Right Plot
===

Next we give the posterior distribution of $\beta_3$ along with the prior distribution for comparison. 

The 95% quantile-based posterior confidence interval for $\beta_3$  is $(-0.026, 0.178)$ which contains zero but still represents a reasonable amount of evidence that the slope for the $x_2 = 1$ is larger than the $x_2 = 0$ group.





