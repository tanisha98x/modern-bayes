
---
title: "Module 7: Part III: Gibbs Sampling and Data Augmentation"
author: "Rebecca C. Steorts"
output: 
     beamer_presentation:
      includes: 
          in_header: custom2.tex
font-size: 8px
---

Agenda
===
- Two Component Mixture Model
- Latent Variable Allocation (Trick for Gibbs Sampling)
- Application to the Dutch Example

Goal
===

The goal of this module is to introduce **mixture models**, which are commonly used in applications to networks analyses, cleaning data (entity resolution), neuroscience, and many other real case studies. 

We will do this using a **latent variable**.

Structure of Module
===

Then we will utilize concepts that we have learned throughout the semester which include:

- a case study to Dutch heights
- hierarchical modeling
- semi-conjugacy 
- Gibbs sampling
- MCMC diagnostics
- approximating posterior distributions and explaning our results
- performing sensitivity analyses

Background (Recall)
===

A **latent variable** is the true version of the state of a random variable that is unknown and not directly observed. 

Latent Variable or Data Augmentation Approach 
===

A commonly-used technique for designing MCMC samplers is to use \emph{data augmentation}, which involves introducing a latent variable into the model. 

\begin{itemize}
\item Introduce variable(s) $Z$ that depend on the distribution of the existing variables in such a way that the resulting conditional distributions, with $Z$ included, are easier to sample from and/or result in better mixing.
\item $Z$'s are latent/auxiliary/hidden variables that are introduced for the purpose of simplifying/improving the sampler.
\end{itemize}

Idea: Create Z's and throw them away at the end! 
===
Goal: Sample from $p(x,y)$

Problem: We cannot sample from $p(x|y)$ and/or $p(y|x).$

Solution: 
\begin{enumerate}
\item Introduce a latent/hidden variable $Z$ such that 
$$p(x|y,z), p(y|x,z),  p(z|x,y)$$ are easy to sample from. 
\item Then construct a Gibbs sampler that will approximate samples from
$p(x,y,z)$ using the full conditionals above. 
\item The output of the three-stage Gibbs sampler is $(X,Y,Z).$
\end{enumerate}

But, our main interest is sampling from $p(x,y).$ Thus, we just throw away the $Z$'s and we will have samples $(X,Y)$ from $p(x,y)$.

Dutch Example
===
Consider a data set on the heights of 695 Dutch women and 562 Dutch men.  

\vskip 1em

Suppose we have the list of heights, but we don't know which data points
are from women and which are from men.  

Dutch Example 
===
From Figure \ref{figure:heights-combined} can we still infer the distribution of female heights and male heights?

\begin{figure}
  \begin{center}
    \includegraphics[width=1\textwidth]{examples/heights-combined}
    % Source: Original work by J. W. Miller.
  \end{center}
  \caption{Heights of Dutch women and men, combined.}
  \label{figure:heights-combined}
\end{figure}
Surprisingly, the answer is yes!

Dutch example
===
What's the magic trick?
\vskip 1em

The reason is that this is a two-component mixture of Normals,
and there is an (essentially) unique set of mixture parameters corresponding to any such distribution.

\vskip 1em

We'll get to details soon. Be patient!

Constructing a Gibbs sampler
===
To construct a Gibbs sampler for this situation:

\begin{itemize}
\item Common to introduce an auxiliary variable $Z_i$ for each data point,
indicating which mixture component it is drawn from.
\item  In this example, $Z_i$ indicates whether subject $i$ is female or male.
\item This results in a Gibbs sampler that is easy to derive/implement.
\end{itemize}

Two-component mixture model
===
Assume that both mixture components (female and male) have the same precision,  $\lambda$, which is fixed and known. Assume below that $a,b,m, \ell$ are known.
Let $\pi$ be the prior probability that a subject comes from component 1.

\vskip 1em

Then the two-component Normal mixture model is:
\begin{align}
  & X_1,\ldots, X_n \mid \mu,\pi\ \sim F(\mu,\pi)\\
      & \mu:=(\mu_0,\mu_1) \stackrel{iid}{\sim} \N(m,\ell^{-1})\\
        & \textcolor{blue}{\pi} \sim \Beta(a,b),
\end{align}
where $F(\mu,\pi)$ is the distribution with p.d.f. 
$$ f(x|\mu,\textcolor{blue}{\pi}) = (1-\textcolor{blue}{\pi})\N(x\mid \mu_0,\lambda^{-1}) + \textcolor{blue}{\pi}
\N(x\mid \mu_1,\lambda^{-1}).$$







Likelihood
===
The likelihood is
\begin{align*}
    p(x_{1:n}|\mu,\pi) &= \prod_{i=1}^n f(x_i|\mu,\pi) \\
                       & = \prod_{i=1}^n \Big[ (1-\pi)\N(x_i\mid \mu_0,\lambda^{-1}) + \pi\N(x_i\mid \mu_1,\lambda^{-1}) \Big].
\end{align*}

Likelihood
===

\textcolor{blue}{What do you notice about the likelihood function?}

\begin{align*}
    p(x_{1:n}|\mu,\pi) &= \prod_{i=1}^n f(x_i|\mu,\pi) \\
                       & = \prod_{i=1}^n \Big[ (1-\pi)\N(x_i\mid \mu_0,\lambda^{-1}) + \pi\N(x_i\mid \mu_1,\lambda^{-1}) \Big].
\end{align*}


Likelihood
===
The **likelihood** is very complicated function of $\mu$ and $\pi$. 

\vspace*{1em}

This makes the posterior difficult to sample from directly.

\vspace*{1em}

Thus, we will rewrite the likelihood using **latent variables**. 

Latent allocation variables to the rescue!
===
Define an equivalent model that includes latent ``allocation'' variables $Z_1,\ldots,Z_n.$
\vskip 1em

These indicate which mixture component each data point
comes from--that is, $Z_i$ indicates whether subject $i$ is female or male.
\begin{align}
 & X_i \mid \mu, Z \sim\N(\mu_{Z_i},\lambda^{-1}) \text{ independently for } i=1,\ldots,n.\\
 & Z_1,\ldots,Z_n \mid \mu,\pi\,\stackrel{iid}{\sim}\,\Bernoulli(\pi)\\
  & \mu= (\mu_0,\mu_1) \stackrel{iid}{\sim}\N(m,\ell^{-1})\\
    & \pi \sim \Beta(a,b)
\end{align}

How can we check that the latent allocation model is equivalent to our original model? This is left as a practice exercise for Exam II. (Solution is at the end of the module.)

Full conditionals
===
Recall
\begin{align*}
 & X_i \mid \mu, Z\sim \N(\mu_{Z_i},\lambda^{-1}) \text{ independently for } i=1,\ldots,n.\\
     & \textcolor{blue}{Z_1,\ldots,Z_n|\mu,\pi\,\stackrel{iid}{\sim}\,\Bernoulli(\pi)}\\
    & \mu= (\mu_0,\mu_1) \stackrel{iid}{\sim} \N(m,\ell^{-1})\\
    & \textcolor{blue}{\pi \sim \Beta(a,b)}
\end{align*}

\begin{itemize}
    \item ($\pi|\cdots$) Given $z$, $\pi$ is independent of everything else, so this reduces to a Beta--Bernoulli model, and we have
        $$ p(\pi|\mu,z,x) = p(\pi|z) =\Beta(\pi\mid a + n_1,\, b + n_0) $$
        where $n_k = \sum_{i=1}^n \I(z_i=k)$ for $k \in \{0, 1\}$.
        \end{itemize}
        \textcolor{blue}{That is, $n_o = \sum_{i=1}^n \I(z_i=0); n_1 = \sum_{i=1}^n \I(z_i=1).$}
        
Full conditionals
===
\begin{align*}
 & \textcolor{blue}{X_i\sim\N(\mu_{Z_i},\lambda^{-1}) \text{ independently for } i=1,\ldots,n.}\\
     & Z_1,\ldots,Z_n|\mu,\pi\,\stackrel{iid}{\sim}\,\Bernoulli(\pi)\\
    & \textcolor{blue}{\mu= (\mu_0,\mu_1) \stackrel{iid}{\sim} \N(m,\ell^{-1})}\\
    & \pi \sim \Beta(a,b)
\end{align*}

\begin{itemize}        
    \item ($\mu|\cdots$) Given $z$, we know which component each data point comes from. 
    The model (conditionally on $z$) is just two
        independent Normal--Normal models, as we have seen before:
        \begin{align*}
            \bm\mu_0|\mu_1,x,z,\pi\, &\sim \,\N(M_0,L_0^{-1})\\
            \bm\mu_1|\mu_0,x,z,\pi\, &\sim \,\N(M_1,L_1^{-1}), \text{where}
        \end{align*}
    \end{itemize}    
         \textcolor{blue}{$$L_0 =\ell + n_0\lambda; L_1 =\ell + n_1\lambda$$
         $$M_0 =\frac{\ell m + \lambda\sum_{i:z_i=0} x_i}{\ell + n_0\lambda}.$$
         $$M_1 =\frac{\ell m + \lambda\sum_{i:z_i=1} x_i}{\ell + n_1\lambda}.$$}
This derivation can be found in Chapter 3.
        
Full conditionals
===
\begin{itemize}
    \item ($z|\cdots$)
        \begin{align*}
            p(z|\mu,\pi,x)&\underset{z}{\propto}p(x,z,\pi,\mu)\underset{z}{\propto} p(x|z,\mu) p(z|\pi)\\
            & =\prod_{i = 1}^n \N(x_i|\mu_{z_i},\lambda^{-1})\Bernoulli(z_i|\pi)\footnote{Not multiplication here due to dependence of $z_i$}\\
            & =\prod_{i = 1}^n \Big(\pi\N(x_i|\mu_1,\lambda^{-1})\Big)^{z_i} \Big((1-\pi)\N(x_i|\mu_0,\lambda^{-1})\Big)^{1-z_i}\\
            & =\prod_{i = 1}^n \alpha_{i,1}^{z_i} \alpha_{i,0}^{1-z_i}\\
            &\underset{z}{\propto}\prod_{i = 1}^n\Bernoulli(z_i\mid \alpha_{i,1}/(\alpha_{i,0}+\alpha_{i,1}))
        \end{align*}
        where
        \begin{align*}
            \alpha_{i,0} & =(1-\pi)\N(x_i|\mu_0,\lambda^{-1})\\
            \alpha_{i,1} & =\pi\N(x_i|\mu_1,\lambda^{-1}).
        \end{align*}
\end{itemize}



My Factory Settings!
===

We initialize the sampler at the same settings that we did when we looked at this application before. Let's review them below. 

\begin{itemize}
\item $\lambda = 1/\sigma^2$ where $\sigma = 8$ cm ($\approx 3.1$ inches) ($\sigma$ = standard deviation of the subject heights within each component)
\item $a = 1$, $b = 1$ (Beta parameters, equivalent to prior ``sample size'' of 1 for each component)
\item $m = 175$ cm ($\approx 68.9$ inches) (mean of the prior on the component means)
\item $\ell = 1/s^2$ where $s = 15$ cm ($\approx 6$ inches) ($s$ = standard deviation of the prior on the component means)
\end{itemize}

My Factory Settings!
===

We initialize the sampler at the same settings that we did when we looked at this application before. Let's review them below. 

\begin{itemize}
\item $\pi = 1/2$ (equal probability for each component)
\item $z_1,\ldots,z_n$ sampled i.i.d.\ from $\Bernoulli(1/2)$ (initial assignment to components chosen uniformly at random)
\item $\mu_0 =\mu_1 = m$ (component means initialized to the mean of their prior)
\end{itemize}


Traceplots of the component means
===
\begin{figure}[htbp]
\begin{center}
\includegraphics[width=\textwidth]{examples/mix-mu_trace-a.png}\caption{Traceplots of the component means, $\mu_0$ and $\mu_1$. The blue component quickly settles to have a mean of around 168–170 cm and the other (red) to a mean of around 182–186 cm.}
\label{default}
\end{center}
\end{figure}

Recall we're not using the true assignments of subjects to components (we don’t know whether they are male or female). 
Note that this is fairly close to the sample averages: 168.0 cm (5 feet 6.1 inches) for females, and 181.4 cm (5 feet 11.4 inches) for males. 

Traceplots of the mixture weight, $\pi$
===    
\begin{figure}
\begin{center}
\includegraphics[width=\textwidth]{examples/mix-p_trace-a.png}
\caption{Traceplots of the mixture weight, $\pi$ (prior probability that a subject comes from component 1). }
\label{figure:mixb}
\end{center}  
\end{figure}

The traceplot of $\pi$ indicates that the sampler is exploring values of around 0.2 to 0.4—that is, the proportion of people coming from group 1 is around 0.2 to 0.4.

Looking at the actual labels (female and male), the empirical proportion of males is $562/(695 + 562) \approx 0.45,$ so this is 
slight off. 

How might we fix this in our model? 

Comceptual Question
===

Recall
\begin{align*}
 & X_i \mid \mu, Z \sim\N(\mu_{Z_i},\lambda^{-1}) \text{ independently for } i=1,\ldots,n.\\
     & Z_1,\ldots,Z_n|\mu,\pi\,\stackrel{iid}{\sim}\,\Bernoulli(\pi)\\
    & \mu= (\mu_0,\mu_1) \stackrel{iid}{\sim} \N(m,\ell^{-1})\\
    & \pi \sim \Beta(a,b),
\end{align*}

where $a,b,m,\ell$ are fixed.

Hint: Is it reasonable to think that we should have the same $\lambda$ for the data?
Consider $$\lambda = (\lambda_0, \lambda_1)$$

How would you proceed next in a Bayesian hierarchical manner? 


Histograms
===
\begin{figure}[htbp]
\begin{center}
\includegraphics[width=\textwidth]{examples/mix-histograms_at_last_sample-a.png}\caption{Histograms of the heights of subjects assigned to each component, according to $z_1,\ldots,z_n$, in a typical sample.}
\label{default}
\end{center}
\end{figure}


Question
===

Why are females assigned to component 0 and males assigned to component 1? 
Why not the other way around? Let's investigate this and see what happens if we look at another choice of starting values. 
    
Traceplots of the component means
===
\begin{figure}[htbp]
\begin{center}
\includegraphics[width=\textwidth]{examples/mix-mu_trace-a.png}
\includegraphics[width=\textwidth]{examples/mix-mu_trace-b.png}\caption{Traceplots of the component means, $\mu_0$ and $\mu_1$.}
\label{default}
\end{center}
\end{figure} 

Traceplots of the mixture weight, $\pi$
===    
\begin{figure}
\begin{center}
\includegraphics[width=\textwidth]{examples/mix-p_trace-a.png}
\includegraphics[width=\textwidth]{examples/mix-p_trace-b.png}
\caption{Traceplots of the mixture weight, $\pi$ (prior probability that a subject comes from component 1). }
\label{figure:mixb}
\end{center}  
\end{figure}

Histograms
===
\begin{figure}[htbp]
\begin{center}
\includegraphics[width=\textwidth]{examples/mix-histograms_at_last_sample-a.png}
\includegraphics[width=\textwidth]{examples/mix-histograms_at_last_sample-b.png}\caption{Histograms of the heights of subjects assigned to each component, according to $z_1,\ldots,z_n$, in a typical sample.}
\label{default}
\end{center}
\end{figure}


Caution: watch out for modes
===
Example illustrates a big thing that can go wrong with MCMC (although fortunately, in this case, the results are still valid if interpreted correctly). 
\begin{itemize}
\item Why are females assigned to component 0 and males assigned to component 1? Why not the other way around? 
\item In fact, the model is symmetric with respect to the two components, and thus the posterior is also symmetric. 
\item If we run the sampler multiple times (starting from the same initial values), sometimes it will settle on females as 0 and males as 1, and sometimes on females as 1 and males as 0 --- see Figure \ref{figure:mixb}. 
\item Roughly speaking, the posterior has two modes.
\item  If the sampler were behaving properly, it would move back and forth between these two modes.
\item But it doesn't---it gets stuck in one and stays there.
\end{itemize}

# Detailed Takeaways

- Latent variable
- Mixture models
- Model for Dutch case study
- Why we need to use a latent variable 
- Deriving conditional distributions
- Understanding the Gibbs sampler updates
- Understanding conceptual questions that relate to the case study 
- What are the main take aways?
- What are benefits of this model? How could you improve the model? 

Takeaways from Dutch Application
===

- This is a very common problem with mixture models. 
- Fortunately, however, in the case of mixture models, the results are still valid if we interpret them correctly. 
- Specifically, our inferences will be valid as long as we only consider quantities that are invariant with respect to permutations of the components (e.g. symmetry about the mean, others).
- To learn more about invariance and what quantities remain unchanged, see Theory of Point Estimation, Lehmann and Casella, Chapter 1 for a formal treatment. (For more advanced reading, see Chapter 3.) Feel free to see me in OH if you want to chat about this! 


Equivalence of both models (Practice Exercise for Exam II)
===
Recall
\begin{align*}
 & X_i \mid \mu, Z \sim\N(\mu_{Z_i},\lambda^{-1}) \text{ independently for } i=1,\ldots,n.\\
     & Z_1,\ldots,Z_n|\mu,\pi\,\stackrel{iid}{\sim}\,\Bernoulli(\pi)\\
    & \mu= (\mu_0,\mu_1) \stackrel{iid}{\sim} \N(m,\ell^{-1})\\
    & \pi \sim \Beta(a,b)
\end{align*}

This is equivalent to the model above, since
\begin{align}
    &p(x_i|\mu,\pi) \\
    &= p(x_i|Z_i=0,\mu,\pi)\Pr(Z_i=0|\mu,\pi)
                    + p(x_i|Z_i=1,\mu,\pi)\Pr(Z_i=1|\mu,\pi) \\
            &= (1-\pi)\N(x_i|\mu_0,\lambda^{-1}) + \pi\N(x_i|\mu_1,\lambda^{-1})\\
            &= f(x_i|\mu,\pi),
\end{align}
and thus it induces the same distribution on $(x_{1:n},\mu,\pi)$. The latent model is considerably easier to work with, particularly for Gibbs sampling.





